{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BirdCLEF 2022 based on Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T11:11:43.476405Z",
     "iopub.status.busy": "2022-05-12T11:11:43.476008Z",
     "iopub.status.idle": "2022-05-12T11:11:57.153139Z",
     "shell.execute_reply": "2022-05-12T11:11:57.151986Z",
     "shell.execute_reply.started": "2022-05-12T11:11:43.476303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Install python_speech_features to calculate filter banks features:\\n!ls ../input/python-speech-features06-zip/\\n!mkdir -p /tmp/pip/cache/\\n!cp ../input/python-speech-features06-zip/python_speech_features-0.6.xyz /tmp/pip/cache/python_speech_features-0.6.tar.gz\\n!pip install --no-index --find-links /tmp/pip/cache/ python_speech_features'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Install python_speech_features to calculate filter banks features:\n",
    "!ls ../input/python-speech-features06-zip/\n",
    "!mkdir -p /tmp/pip/cache/\n",
    "!cp ../input/python-speech-features06-zip/python_speech_features-0.6.xyz /tmp/pip/cache/python_speech_features-0.6.tar.gz\n",
    "!pip install --no-index --find-links /tmp/pip/cache/ python_speech_features\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-12T11:11:57.160334Z",
     "iopub.status.busy": "2022-05-12T11:11:57.159734Z",
     "iopub.status.idle": "2022-05-12T11:12:05.434057Z",
     "shell.execute_reply": "2022-05-12T11:12:05.432825Z",
     "shell.execute_reply.started": "2022-05-12T11:11:57.160290Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import python_speech_features\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T11:12:05.436399Z",
     "iopub.status.busy": "2022-05-12T11:12:05.436023Z",
     "iopub.status.idle": "2022-05-12T11:12:05.445093Z",
     "shell.execute_reply": "2022-05-12T11:12:05.443403Z",
     "shell.execute_reply.started": "2022-05-12T11:12:05.436340Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data:\n",
    "TRAIN_DIR = 'train_audio/'\n",
    "SAMPLE_RATE = 32000\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# Data processing:\n",
    "WINDOW_LENGTH = 0.25\n",
    "WINDOW_STRIDE = 0.01\n",
    "N_MELS = 32\n",
    "N_FFT = 512\n",
    "FMIN = 0\n",
    "FMAX = SAMPLE_RATE / 2\n",
    "PREEMPHASIS_COEFFICIENT = 0.97\n",
    "STRIDE = 14\n",
    "\n",
    "# Learning process:\n",
    "NAME_MODEL_0 = \"model_0/model_0.h5\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "CALL_BACKS = None\n",
    "CHUNK_SIZE = 316"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T11:12:05.449061Z",
     "iopub.status.busy": "2022-05-12T11:12:05.448507Z",
     "iopub.status.idle": "2022-05-12T11:12:05.599659Z",
     "shell.execute_reply": "2022-05-12T11:12:05.598456Z",
     "shell.execute_reply.started": "2022-05-12T11:12:05.449015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-1.4930</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>08:00</td>\n",
       "      <td>https://www.xeno-canto.org/125458</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['houspa', 'redava', 'zebdov']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>19.8801</td>\n",
       "      <td>-155.7254</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Dan Lane</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/175522</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>16.2901</td>\n",
       "      <td>-16.0321</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>https://www.xeno-canto.org/177993</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>17.0922</td>\n",
       "      <td>54.2958</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Oscar Campbell</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:00</td>\n",
       "      <td>https://www.xeno-canto.org/205893</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>21.4581</td>\n",
       "      <td>-157.7252</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Ross Gallardy</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16:30</td>\n",
       "      <td>https://www.xeno-canto.org/207431</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                secondary_labels                     type  \\\n",
       "0       afrsil1                              []  ['call', 'flight call']   \n",
       "1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n",
       "2       afrsil1                              []         ['call', 'song']   \n",
       "3       afrsil1                              []   ['alarm call', 'call']   \n",
       "4       afrsil1                              []          ['flight call']   \n",
       "\n",
       "   latitude  longitude  scientific_name         common_name          author  \\\n",
       "0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n",
       "1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n",
       "2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n",
       "3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n",
       "4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n",
       "\n",
       "                                 url              filename  \n",
       "0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg  \n",
       "1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg  \n",
       "2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg  \n",
       "3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg  \n",
       "4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metadata = pd.read_csv('train_metadata.csv')\n",
    "train_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T11:12:05.603781Z",
     "iopub.status.busy": "2022-05-12T11:12:05.603342Z",
     "iopub.status.idle": "2022-05-12T11:12:05.727246Z",
     "shell.execute_reply": "2022-05-12T11:12:05.725928Z",
     "shell.execute_reply.started": "2022-05-12T11:12:05.603745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load work classes:\n",
    "with open('scored_birds.json', 'r') as f:\n",
    "    valid_classes = json.load(f)\n",
    "valid_classes.append('other')\n",
    "N_CLASSES = len(valid_classes)\n",
    "\n",
    "# Encode labels:\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "valid_labels = encoder.fit_transform(valid_classes)\n",
    "\n",
    "edited_labels = list()\n",
    "primary_labels = train_metadata.primary_label\n",
    "for i in range(primary_labels.shape[0]):\n",
    "    if primary_labels[i] in valid_classes:\n",
    "        edited_labels.append(primary_labels[i])\n",
    "    else:\n",
    "        edited_labels.append('other')\n",
    "labels = encoder.transform(edited_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T11:12:05.730917Z",
     "iopub.status.busy": "2022-05-12T11:12:05.730307Z",
     "iopub.status.idle": "2022-05-12T11:12:05.757009Z",
     "shell.execute_reply": "2022-05-12T11:12:05.755669Z",
     "shell.execute_reply.started": "2022-05-12T11:12:05.730863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cut the signal into frames duration 5 sec:\n",
    "def framing(sig: np.ndarray, sample_rate: int, frame_len: int, duration_time: float) -> np.ndarray:\n",
    "    num_frames = int(np.ceil(duration_time / 5))\n",
    "    framed_sig = np.zeros((num_frames, int(frame_len * sample_rate)))\n",
    "    start_time = 0\n",
    "    end_time = frame_len * sample_rate\n",
    "    if duration_time < 5:\n",
    "        framed_sig[0][:sig.shape[0]] = sig\n",
    "    else:\n",
    "        for i in range(num_frames):\n",
    "            framed_sig[i][:end_time - start_time] = sig[start_time:end_time]\n",
    "            start_time = start_time + int(frame_len * sample_rate)\n",
    "            if i == num_frames - 2:\n",
    "                end_time = end_time + int(sig.shape[0] - start_time)\n",
    "            else:\n",
    "                end_time = end_time + int(frame_len * sample_rate)\n",
    "\n",
    "    return framed_sig\n",
    "\n",
    "\n",
    "def processingChunkAudio(filenames: list, labels: np.ndarray) -> list:\n",
    "    features_arr = np.zeros((1, STRIDE, N_MELS, N_MELS, 1))\n",
    "    labels_arr = np.zeros((1, STRIDE, N_CLASSES))\n",
    "    \n",
    "    for i in range(len(filenames)):\n",
    "        # Load audio:\n",
    "        signal, _ = librosa.load(\n",
    "            TRAIN_DIR + filenames.iloc[i],\n",
    "            sr=SAMPLE_RATE,\n",
    "            mono=True,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Divide signal into frames duration 5 sec:\n",
    "        frames = framing(\n",
    "            sig=signal,\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            frame_len=5,\n",
    "            duration_time=librosa.get_duration(\n",
    "                y=signal,\n",
    "                sr=SAMPLE_RATE\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for j in range(frames.shape[0]):\n",
    "        # Extract log filter banks:\n",
    "            mel = python_speech_features.base.logfbank(\n",
    "                frames[j],\n",
    "                samplerate=SAMPLE_RATE,\n",
    "                winlen=WINDOW_LENGTH,\n",
    "                winstep=WINDOW_STRIDE,\n",
    "                nfilt=N_MELS,\n",
    "                nfft=N_FFT,\n",
    "                lowfreq=FMIN,\n",
    "                highfreq=FMAX,\n",
    "                preemph=PREEMPHASIS_COEFFICIENT\n",
    "            )\n",
    "            mel = np.float32(mel)\n",
    "        \n",
    "            # Make images:\n",
    "            num_images = int(np.floor(mel.shape[0] / N_MELS))\n",
    "            mel = mel[:num_images * N_MELS]\n",
    "            images = mel.reshape(num_images, N_MELS, N_MELS)\n",
    "        \n",
    "            # Make series of images:\n",
    "            num_groups = int(np.floor(images.shape[0] / STRIDE))\n",
    "            images = images[:num_groups * STRIDE]\n",
    "            sequences = images.reshape(num_groups, STRIDE, images.shape[1], images.shape[2], 1)\n",
    "        \n",
    "            # Reshape and convert label for images:\n",
    "            label_cat = tf.keras.utils.to_categorical(\n",
    "                labels[i],\n",
    "                num_classes=N_CLASSES,\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "            labels_reshaped = np.full(\n",
    "                shape=(sequences.shape[0], sequences.shape[1], N_CLASSES),\n",
    "                fill_value=label_cat,\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "        \n",
    "            # Stack series of images and labels into arrays:\n",
    "            features_arr = np.vstack((features_arr, sequences))\n",
    "            labels_arr = np.vstack((labels_arr, labels_reshaped))\n",
    "        \n",
    "            # Erase memory:\n",
    "            del mel\n",
    "            del num_images\n",
    "            del images\n",
    "            del num_groups\n",
    "            del sequences\n",
    "            del label_cat\n",
    "            del labels_reshaped\n",
    "            gc.collect()\n",
    "        \n",
    "        # Erase memory:\n",
    "        del signal\n",
    "        del frames\n",
    "        gc.collect()\n",
    "    \n",
    "    # Erase memory:\n",
    "    del filenames\n",
    "    del labels\n",
    "    gc.collect()\n",
    "    \n",
    "    # Delete the first zero element:\n",
    "    features_arr = features_arr[1:]\n",
    "    labels_arr = labels_arr[1:]\n",
    "    \n",
    "    return [features_arr, labels_arr]\n",
    "\n",
    "\n",
    "def splitData(data: list) -> list:\n",
    "    # Split into train and val sets:\n",
    "    train_features = data[0][:int(TRAIN_SIZE * data[0].shape[0])]\n",
    "    train_labels = data[1][:int(TRAIN_SIZE * data[1].shape[0])]\n",
    "    val_features = data[0][int(TRAIN_SIZE * data[0].shape[0]):]\n",
    "    val_labels = data[1][int(TRAIN_SIZE * data[1].shape[0]):]\n",
    "    \n",
    "    # Erase memory:\n",
    "    del data\n",
    "    \n",
    "    return [train_features, train_labels, val_features, val_labels]\n",
    "\n",
    "\n",
    "def dataGenerator(features: np.ndarray, labels: np.ndarray):\n",
    "    # Make tf dataset:\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    # Erase memory:\n",
    "    del features\n",
    "    del labels\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T11:12:05.759265Z",
     "iopub.status.busy": "2022-05-12T11:12:05.758832Z",
     "iopub.status.idle": "2022-05-12T11:12:05.779032Z",
     "shell.execute_reply": "2022-05-12T11:12:05.777628Z",
     "shell.execute_reply.started": "2022-05-12T11:12:05.759216Z"
    }
   },
   "outputs": [],
   "source": [
    "def getModel_0():\n",
    "    layer_input = tf.keras.Input((STRIDE, N_MELS, N_MELS, 1), dtype=tf.float32)\n",
    "    layer_model_0 = tf.keras.layers.BatchNormalization()(layer_input)\n",
    "    layer_model_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(64, (5, 5), activation='elu'))(layer_model_0)\n",
    "    layer_model_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2)))(layer_model_0)\n",
    "    layer_model_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Conv2D(128, (3, 3), activation='elu'))(layer_model_0)\n",
    "    layer_model_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D((2, 2)))(layer_model_0)\n",
    "    layer_model_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())(layer_model_0)\n",
    "    layer_model_0 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='elu'))(layer_model_0)\n",
    "    layer_model_0 = tf.keras.layers.Dropout(0.5)(layer_model_0)\n",
    "    layer_model_0 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(layer_model_0)\n",
    "    layer_model_0 = tf.keras.layers.Dropout(0.5)(layer_model_0)\n",
    "    layer_output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(valid_classes), activation='softmax'))(layer_model_0)\n",
    "    model_0 = tf.keras.Model(inputs=[layer_input], outputs=[layer_output])\n",
    "    model_0.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-12T11:12:05.782734Z",
     "iopub.status.busy": "2022-05-12T11:12:05.781380Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_metadata = train_metadata.sample(frac=1, random_state=42)\n",
    "labels = tf.random.shuffle(labels, seed=42)\n",
    "\n",
    "model_0 = getModel_0()\n",
    "train_log_los_err = np.zeros(EPOCHS)\n",
    "val_log_loss_err = np.zeros(EPOCHS)\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    for i in range(int(train_metadata.shape[0] / CHUNK_SIZE)):\n",
    "        # Prepare data for fitting:\n",
    "        processed_data = processingChunkAudio(\n",
    "            filenames=train_metadata.filename.iloc[i * CHUNK_SIZE:(i + 1) * CHUNK_SIZE],\n",
    "            labels=labels[i * CHUNK_SIZE:(i + 1) * CHUNK_SIZE]\n",
    "        )\n",
    "        splitted_data = splitData(data=processed_data)\n",
    "        train_ds = dataGenerator(features=splitted_data[0], labels=splitted_data[1])\n",
    "        val_ds = dataGenerator(features=splitted_data[2], labels=splitted_data[3])\n",
    "        \n",
    "        # Fit the model:\n",
    "        history = model_0.fit(train_ds, validation_data=val_ds, epochs=1, batch_size=BATCH_SIZE, callbacks=CALL_BACKS)\n",
    "        train_log_los_err[epoch] = train_log_los_err[epoch] + history.history['loss'][0]\n",
    "        val_log_loss_err[epoch] = val_log_loss_err[epoch] + history.history['val_loss'][0]\n",
    "        \n",
    "        # Erase memory:\n",
    "        del processed_data\n",
    "        del splitted_data\n",
    "        del train_ds\n",
    "        del val_ds\n",
    "        del model_0\n",
    "        gc.collect()\n",
    "    \n",
    "    train_log_los_err[epoch] = train_log_los_err[epoch] / int(train_metadata.shape[0] / CHUNK_SIZE)\n",
    "    val_log_loss_err[epoch] = val_log_loss_err[epoch] / int(train_metadata.shape[0] / CHUNK_SIZE)\n",
    "    \n",
    "model_0.save(NAME_MODEL_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
